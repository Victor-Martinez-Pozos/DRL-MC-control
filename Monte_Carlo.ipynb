{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils.plot_utils import plot_blackjack_values, plot_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code cell below to create an instance of the [Blackjack](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py) environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each state is a 3-tuple of:\n",
    "- the player's current sum $\\in \\{0, 1, \\ldots, 31\\}$,\n",
    "- the dealer's face up card $\\in \\{1, \\ldots, 10\\}$, and\n",
    "- whether or not the player has a usable ace (`no` $=0$, `yes` $=1$).\n",
    "\n",
    "The agent has two potential actions:\n",
    "\n",
    "```\n",
    "    STICK = 0\n",
    "    HIT = 1\n",
    "```\n",
    "Verify this by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, False)\n",
      "End game! Reward:  -1.0\n",
      "You lost :(\n",
      "\n",
      "(10, 9, False)\n",
      "(12, 9, False)\n",
      "End game! Reward:  -1.0\n",
      "You lost :(\n",
      "\n",
      "(15, 7, False)\n",
      "End game! Reward:  1.0\n",
      "You won :)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MC Control\n",
    "\n",
    "In this section, you will write your own implementation of constant-$\\alpha$ MC control.  \n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(bj_env, policy=None):\n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    nA = bj_env.action_space.n\n",
    "    while True:\n",
    "        if (state) in policy:\n",
    "            action = policy[state]\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(nA))\n",
    "        \n",
    "        next_state, reward, done, info = bj_env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_return(gamma, rewards):\n",
    "    reward = rewards.pop(0)\n",
    "    if len(rewards) == 0: \n",
    "        return reward\n",
    "    else:\n",
    "        return(reward + gamma*get_discounted_return(gamma, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, epsilon):\n",
    "    policy={}\n",
    "    for state, action in Q.items():\n",
    "        best_index = np.argmax(action)\n",
    "        n_a = len(action)\n",
    "        \n",
    "        non_greedy_prob = epsilon/n_a      \n",
    "        probs=np.ones_like(action)*non_greedy_prob\n",
    "        \n",
    "        greedy_prob = 1-epsilon + (epsilon/n_a)\n",
    "        probs[best_index] = greedy_prob\n",
    "        \n",
    "        policy[state]= np.random.choice(np.arange(action.size), p=probs)\n",
    "        \n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=0.9, epsilon=1.0):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):       \n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "           \n",
    "        epsilon_i = epsilon - ((i_episode-1)/(num_episodes-1))\n",
    "        policy = epsilon_greedy_policy(Q, epsilon_i)\n",
    "            \n",
    "        # generate an episode using new policy\n",
    "        episode=generate_episode(env, policy)\n",
    "        \n",
    "        # Iterate over transitions\n",
    "        states, actions, rewards = zip(*episode)      \n",
    "        actions=list(actions)\n",
    "        rewards=list(rewards)\n",
    "        \n",
    "        episode_first_counter={}\n",
    "        for t_i, state in enumerate(states):\n",
    "            action = actions.pop(0)           \n",
    "            # Check if its first visit\n",
    "            if not ((state, action) in episode_first_counter):\n",
    "                episode_first_counter[(state, action)] = 1\n",
    "                \n",
    "                G_t=get_discounted_return(gamma, rewards.copy())\n",
    "                Q[state][action] += alpha*(G_t-Q[state][action])\n",
    "\n",
    "            rewards.pop(0)\n",
    "            \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to obtain the estimated optimal policy and action-value function.  Note that you should fill in your own values for the `num_episodes` and `alpha` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32000/100000."
     ]
    }
   ],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy, Q = mc_control(env, num_episodes=100000, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the corresponding state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# obtain the corresponding state-value function\n",
    "V = dict((k,np.max(v)) for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the policy that is estimated to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the policy\n",
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **true** optimal policy $\\pi_*$ can be found in Figure 5.2 of the [textbook](http://go.udacity.com/rl-textbook) (and appears below).  Compare your final estimate to the optimal policy - how close are you able to get?  If you are not happy with the performance of your algorithm, take the time to tweak the decay rate of $\\epsilon$, change the value of $\\alpha$, and/or run the algorithm for more episodes to attain better results.\n",
    "\n",
    "![True Optimal Policy](images/optimal.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
